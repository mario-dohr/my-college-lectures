{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Assignment 4 - WS 2021 -->"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  \n",
    "may  be  used  for  personal  and non-commercial educational use only.  \n",
    "Any reproduction of this manuscript, no matter whether as a whole or in parts, \n",
    "no matter whether in printed or in electronic form, \n",
    "requires explicit prior acceptance of the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Optimisation (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the fourth assignment for the exercises in Deep Learning and Neural Nets 1.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility function that should work without problems.\n",
    "Please, do not alter this code or add extra import statements in your submission, unless it is explicitly requested!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!\n",
    "\n",
    "In this assignment, the goal is to get familiar with some of the most common (adaptive) **optimisation algorithms**. Essentially, the most common optimisation algorithms are nothing more than variants of gradient descent. Although it is often claimed that stochastic gradient descent outperforms any adaptive learning method when carefully configured, it is often more convenient to use a method that requires less tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nnumpy import Module\n",
    "from nnumpy.data import get_mnist_data\n",
    "from nnumpy.utils import split_data, to_one_hot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    \"\"\"\n",
    "    Compute accuracy for classification network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : ndarray\n",
    "        The logit-predictions from the network.\n",
    "    labels : ndarray\n",
    "        The target labels for the task.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    acc : float\n",
    "        The fraction of correctly classified samples.\n",
    "    \"\"\"\n",
    "    idx = np.argmax(logits, axis=1)\n",
    "    return np.mean(labels[np.arange(len(idx)), idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser:\n",
    "    \"\"\" Base class for NNumpy optimisers. \"\"\"\n",
    "\n",
    "    def __init__(self, parameters, lr: float):\n",
    "        \"\"\"\n",
    "        Create an optimiser instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        parameters : iterable\n",
    "            Iterable over the parameters that need to be updated.\n",
    "        lr : float\n",
    "            Learning rate or step size for updating the parameters.\n",
    "        states : iterable\n",
    "            Iterable over initial update states for each parameter.\n",
    "        \"\"\"\n",
    "        self.parameters = list(parameters)\n",
    "        if len(self.parameters) == 0:\n",
    "            raise ValueError(\"no parameters to optimise\")\n",
    "\n",
    "        self.lr = float(lr)\n",
    "        \n",
    "        self._states = [self.init_state(par) for par in self.parameters]\n",
    "    \n",
    "    def init_state(self, par):\n",
    "        \"\"\"\n",
    "        Create the initial optimiser state for a parameter.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        par : Parameter\n",
    "            The parameter to create the initial state for.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        state : object\n",
    "            The initial optimiser state for the given parameter.\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update all parameters under control of this optimiser\n",
    "        by making one step in the direction computed by the algorithm\n",
    "        for each of the parameters.\n",
    "        \"\"\"\n",
    "        for par, state in zip(self.parameters, self._states):\n",
    "            update = self.get_update(par, state)\n",
    "            par -= self.lr * update\n",
    "            del par.grad  # optional safeguard\n",
    "    \n",
    "    def get_update(self, par, state):\n",
    "        \"\"\"\n",
    "        Compute the update for a single parameter.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        par : Parameter\n",
    "            The parameter (with `.grad` field) to compute the update for.\n",
    "        state : object\n",
    "            The state of the optimiser associated with the parameter.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"method must be implemented in subclass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you should know by now, the *backpropagation* algorithm is little more than a combination of the chain rule and some form of gradient descent. Although this happens to work well in practice, it is good to be aware of possible issues when using first-order optimisation methods:\n",
    "\n",
    " 1. Gradient descent can/will get stuck in *local minima*.\n",
    " 2. The *gradient magnitude* tells you nothing about how far away minima are.\n",
    " 3. When optimising the *empirical error*, gradient descent would require the gradient over the entire dataset.\n",
    "\n",
    "Note that this last point is not necessarily an issue, but it is useful to keep in mind. Also, it implies that the gradients that can be computed on the entire dataset do not need to correspond to the gradients that would be required to minimise the generalisation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Stochasticity (2 Points)\n",
    "\n",
    "Rather than using plain gradient descent algorithm, a stochastic variant is used to train neural networks. This variant is known as *Stochastic Gradient Descent*, or *SGD* for short. Although this naming scheme seems to suggest that stochasticity is part of the algorithm, it is actually introduced by how we use the data to compute gradients.\n",
    "\n",
    "Instead of computing the gradients over the entire dataset in one go, the samples in the dataset are split up in more manageable pieces called *mini-batches*. This can speed up the computations significantly and avoids memory problems with very large datasets. Another benefit from mini-batches is that they introduce variation, or *stochasticity*, in the gradient computations. After all, the gradient for each mini-batch will be different to the gradient for other mini-batches or for all samples. This stochasticity can be useful to escape local minima in the optimisation process. To amplify this stochasticity, it is also common to shuffle the samples in the dataset so that mini-batches consist of different samples.\n",
    "\n",
    "> Complete the `Dataloader` class below to process the data in mini-batches of pre-specified size. Also make sure to shuffle the data to get more stochasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some Notes on python generators\n",
    "\n",
    "In python, a [generator](https://wiki.python.org/moin/Generators) is a function with some state that can return multiple values. You probably have already used generators without realising it. Probably, the most famous generator is `range`, which could be defined as follows:\n",
    "```python\n",
    "def _range(start, stop, step=1):\n",
    "    i = start\n",
    "    while i < stop:\n",
    "        yield i\n",
    "        i += step\n",
    "```\n",
    "\n",
    "Notice the `yield` keyword. This has a similar effect as `return` in that it provides a value to the outer scope of the function. However, it does not cause the function to be exited. Instead, the current state in the function is stored until the next value is requested. To get the return values of a generator, there are essentially two options:\n",
    " 1. Using the `next` function. This will simply run the function until the next `yield` statement and give back the yielded value.\n",
    " 2. By iterating over the generator in any way. This will consequently call `next` on the generator until the function exits.\n",
    " \n",
    "For more information, please refer to the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    \n",
    "    def __init__(self, x, y, batch_size=None, shuffle=False, seed=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, ...) ndarray\n",
    "            the `N` input samples in the dataset\n",
    "        y : (N, ...) ndarray\n",
    "            the `N` output samples in the dataset\n",
    "        batch_size : int, optional\n",
    "            number of samples to include in a single mini-batch.\n",
    "        shuffle : bool, optional\n",
    "            whether or not the data should be shuffled.\n",
    "        seed : int, optional\n",
    "            seed for the pseudo random number generator used for shuffling.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = len(x) if batch_size is None else int(batch_size)\n",
    "        self.shuffle = shuffle\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterates over the samples of the data.\n",
    "        \n",
    "        Yields\n",
    "        ------\n",
    "        x : ndarray\n",
    "            input features for the batch\n",
    "        y : ndarray\n",
    "            target values for the batch\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        Each batch should contain the specified number of samples,\n",
    "        except for the last batch if the batch_size does \n",
    "        not divide the number of samples in the data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"TODO: implement the dataloader.__iter__ function!\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return 1 + (len(self.x) - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "data_loader = Dataloader(np.arange(7), np.arange(7), batch_size=3, shuffle=True)\n",
    "print(len(data_loader))\n",
    "for x, y in data_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Gradient Descent with Momentum (2 Points)\n",
    "\n",
    "Another way to keep gradient descent from getting stuck in local minima is to use momentum. Momentum accumulates the gradient directions over different batches and accelerates/decelarates the descent when all gradients point in the same/different direction(s). This also means that the update does not directly use the magnitude of the gradient, but instead focuses on the direction. \n",
    "\n",
    "> Implement the `get_update` method for the gradient descent optimiser with momentum.\n",
    "\n",
    "**Hint:** you can use in-place operations to change numpy arrays in the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(Optimiser):\n",
    "    \"\"\" NNumpy implementation of gradient descent. \"\"\"\n",
    "\n",
    "    def __init__(self, parameters, lr: float, momentum: float = 0.):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        momentum : float\n",
    "            Momentum term for the gradient descent.\n",
    "        \"\"\"\n",
    "        super().__init__(parameters, lr)\n",
    "        self.mu = momentum\n",
    "\n",
    "    def init_state(self, par):\n",
    "        raise NotImplementedError(\"TODO: implement the GradientDescent.init_state function!\")\n",
    "        \n",
    "    def get_update(self, par, state):\n",
    "        raise NotImplementedError(\"TODO: implement the GradientDescent.get_update function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: The Adam Optimiser (4 Points)\n",
    "\n",
    "Momentum already provides a way to reduce the importance of the gradient magnitude. With adaptive learning rate methods, an attempt is made to ignore most of the magnitude information and the size of the update is mainly controlled by controlling the learning rate. One of the most popular first order adaptive methods in practice, is the Adam optimiser.\n",
    "\n",
    "> Implement the `get_update` method for the Adam optimisation algorithm.\n",
    "\n",
    "**Hint:** you can use in-place operations to change numpy arrays in the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimiser):\n",
    "    \"\"\" NNumpy implementation of the Adam algorithm. \"\"\"\n",
    "\n",
    "    def __init__(self, parameters, lr: float = 1e-3, betas: tuple = (.9, .999),\n",
    "                 epsilon: float = 1e-7, bias_correction=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        betas : tuple of 2 floats, optional\n",
    "            Decay factors for the exponential averaging of mean, resp. variance.\n",
    "        epsilon : float, optional\n",
    "            Small number that is added to denominator for numerical stability.\n",
    "        bias_correction : bool, optional\n",
    "            Whether or not mean and bias estimates should be bias-corrected.\n",
    "        \"\"\"\n",
    "        super().__init__(parameters, lr)\n",
    "\n",
    "        beta1, beta2 = betas\n",
    "        self.beta1 = float(beta1)\n",
    "        self.beta2 = float(beta2)\n",
    "        self.eps = float(epsilon)\n",
    "        self.bias_correction = bias_correction\n",
    "\n",
    "    def init_state(self, par):\n",
    "        raise NotImplementedError(\"TODO: implement the Adam.init_state function!\")\n",
    "\n",
    "    def get_update(self, par, state):\n",
    "        raise NotImplementedError(\"TODO: implement the Adam.get_update function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Evaluation and Update (3 Points)\n",
    "\n",
    "When using the optimisers to fit a neural network to a given set of data, we can effectively minimise the empirical error. However, we are actually interested in minimising the risk. Therefore, it is also useful to evaluate the network regularly on unseen data.\n",
    "\n",
    " > Implement the `evaluate`, and `update` functions so that they perform the training and evaluation computations, respectively, for one iteration (aka *epoch*) over the entire dataset. Make sure to return the loss values to get loss curves at the end of this assignment.\n",
    " \n",
    "**Hint:** you can use the `step` function of the optimiser to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Evaluate a network by computing a metric for specific data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : Module\n",
    "        A module that implements the network.\n",
    "    metric : callable\n",
    "        A function that takes logits and labels \n",
    "        and returns a scalar numpy array.\n",
    "    data_loader : Dataloader\n",
    "        The data loader that provides the batches.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    values : ndarray\n",
    "        The computed metric values for each batch in the data loader.\n",
    "    \"\"\"\n",
    "    network.eval()\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: implement the evaluate function!\")\n",
    "\n",
    "\n",
    "def update(network, loss, data_loader, optimiser):\n",
    "    \"\"\"\n",
    "    Update a network by optimising the loss for the given data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : Module\n",
    "        A module that implements the network.\n",
    "    loss : Module\n",
    "        Loss function module.\n",
    "    data_loader : Dataloader\n",
    "        The data loader that provides the batches.\n",
    "    optimiser : Optimiser\n",
    "        Optimisation algorithm to use for the update.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    errors : ndarray\n",
    "        The computed loss for each batch in the data loader.\n",
    "    \"\"\"\n",
    "    network.train()\n",
    "    loss.train()\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: implement the update function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, valid_data, network, loss, optimiser, \n",
    "          epochs=1, batch_size=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Train a neural network with gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : tuple of ndarrays\n",
    "        Training data as tuple of input features and target values.\n",
    "    valid_data : tuple of ndarrays\n",
    "        Validation data as tuple of input features and target values.\n",
    "    network : Module\n",
    "        A module that implements the network.\n",
    "    loss : Module\n",
    "        Loss function module.\n",
    "    optimiser : Optimiser\n",
    "        Optimisation algorithm.\n",
    "    epochs : int, optional\n",
    "        Number of times to iterate the dataset.\n",
    "    batch_size : int or None, optional\n",
    "        Number of samples to use simultaneously.\n",
    "        If None, all samples are fed to the network.\n",
    "    shuffle : bool, optional\n",
    "        Flag to enable or disable shuffling of the training data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    train_errors : (epochs + 1, n_batches) ndarray\n",
    "        Training error for each epoch and each batch.\n",
    "    valid_errors : (epochs + 1, 1) ndarray\n",
    "        Validation error for each epoch.\n",
    "    \"\"\"\n",
    "    train_loader = Dataloader(*train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    valid_loader = Dataloader(*valid_data, batch_size=None)\n",
    "    \n",
    "    # log performance before training (for reference)\n",
    "    train_errors = [evaluate(network, loss.eval(), train_loader)]\n",
    "    valid_errors = [evaluate(network, loss.eval(), valid_loader)]\n",
    "    # train for given number of epochs\n",
    "    for _ in range(epochs):\n",
    "        train_errors.append(update(network, loss, train_loader, optimiser))\n",
    "        valid_errors.append(evaluate(network, loss.eval(), valid_loader))\n",
    "        \n",
    "    return np.stack(train_errors), np.stack(valid_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Training a Neural Network (4 Points)\n",
    "\n",
    "At this point, we have all ingredients to train fully connected and/or convolutional networks. Time to put your framework to the test and train a neural network to recognise hand-written digits.\n",
    "\n",
    "> Use one of the optimisers from above to train a convolutional neural network on the MNIST dataset. You can use the `get_mnist_data` to download/process the data for this exercise. You will also need to create a neural network, for which you can make use of all modules from previous assignments. You are also allowed to create new modules and try out new things of course (make sure to include any new code in the notebook!). Achieve a model with 80% accuracy to collect all points. For reference: the sample solution (using the LeNet architecture illustrated below) takes &approx;15&nbsp;min to train for 10&nbsp;epochs.\n",
    "\n",
    "**Hint:** Start with small models on a subset of the data. This will save you valuable time when trying out different settings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <figcaption style=\"width: 100%; text-align: center;\">LeNet-5 architecture</figcaption>\n",
    "    <img src=\"https://miro.medium.com/max/2154/1*1TI1aGBZ4dybR6__DI9dzA.png\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules you have written in the previous assignment\n",
    "# or alternatively, the code for these modules, here\n",
    "# e.g. from nnumpy import Sequential, Linear, Conv2D, Tanh, LogitCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(x_train, y_train):\n",
    "    x_train = x_train.reshape(-1, 1, 28, 28)\n",
    "    y_train = to_one_hot(y_train)\n",
    "    return x_train, y_train\n",
    "\n",
    "x_train, y_train = get_mnist_data()\n",
    "x_train, y_train = process_data(x_train, y_train)\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can create additional modules below\n",
    "\n",
    "class Flatten(Module):\n",
    "    \"\"\" NNumpy module to convert multi-dimensional outputs to a single vector. \"\"\"\n",
    "    \n",
    "    def compute_outputs(self, x):\n",
    "        return x.reshape(len(x), -1), x.shape\n",
    "    \n",
    "    def compute_grads(self, grads, shape):\n",
    "        return grads.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"TODO: create a convolutional network and train it!\")\n",
    "train_data, valid_data = ...\n",
    "train_err, valid_err = train(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "plt.title(\"learning curves\")\n",
    "train_acc, = evaluate(cnn, accuracy, Dataloader(*train_data))\n",
    "loss_curve, = plt.plot(np.mean(train_err, axis=1), \n",
    "                       label=f'train (acc: {100 * train_acc:2.2f}%)')\n",
    "valid_acc, = evaluate(cnn, accuracy, Dataloader(*valid_data))\n",
    "plt.plot(valid_err, linestyle='--', color=loss_curve.get_color(), \n",
    "         label=f'valid (acc: {100 * valid_acc:2.2f}%)')\n",
    "plt.ylim(0, 5)  # you can change the upper limit if you want\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}